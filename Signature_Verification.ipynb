{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Signature Verification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Problem Statement -** Forgery at banks has become very common and a serious crime. Here we present a deep learning model to tackle this rising problem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**What have we used??**\n",
    "- We have used *Convolutional Neural Networks* to extract inherent features from a sample signature.\n",
    "- For loss function we have used *Triplet Loss Function* \n",
    "- Model can be trained using *One Shot Learning* but for better results were much better with *Few shot Learning*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Step1:** Import Dependcies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Keras have been used for simlicity of models\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Conv2D, ZeroPadding2D, Activation, Input, concatenate\n",
    "from keras.models import Model\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.layers.pooling import MaxPooling2D, AveragePooling2D\n",
    "from keras.layers.merge import Concatenate\n",
    "from keras.layers.core import Lambda, Flatten, Dense\n",
    "from keras.initializers import glorot_uniform\n",
    "from keras.engine.topology import Layer\n",
    "from keras import backend as K\n",
    "K.set_image_data_format('channels_first') ## setting image format -- "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import os\n",
    "import numpy as np\n",
    "from numpy import genfromtxt\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from fr_utils import *\n",
    "from inception_blocks_v2 import *\n",
    "\n",
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "np.set_printoptions(threshold=np.nan)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step2: Using Pre-Trained Inception Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "FRmodel = faceRecoModel(input_shape=(3, 96, 96))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Params: 3743280\n"
     ]
    }
   ],
   "source": [
    "print(\"Total Params:\", FRmodel.count_params())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step3: Defining Loss function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"triplet_loss.jpg\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Triplet Loss Function:** As the name suggests we calculate difference of positive distance and negative distance. Positive Distance is calculated using anchor and positive while Negative Distance is calculated using anchor and negative image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def triplet_loss(y_true, y_pred, alpha = 0.2):\n",
    "\n",
    "    anchor, positive, negative = y_pred[0], y_pred[1], y_pred[2]\n",
    "    \n",
    "    # distance between the anchor and the positive, you will need to sum over axis=-1\n",
    "    pos_dist = tf.reduce_sum(tf.square(tf.subtract(anchor,positive)))\n",
    "    # distance between the anchor and the negative, you will need to sum over axis=-1\n",
    "    neg_dist = tf.reduce_sum(tf.square(tf.subtract(anchor,negative)))\n",
    "\n",
    "    basic_loss = tf.add(tf.subtract(pos_dist,neg_dist),alpha)\n",
    "\n",
    "    loss = tf.maximum(basic_loss,0)\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss = 350.02716\n"
     ]
    }
   ],
   "source": [
    "## Testing triplet loss function\n",
    "with tf.Session() as test:\n",
    "    tf.set_random_seed(1)\n",
    "    y_true = (None, None, None)\n",
    "    y_pred = (tf.random_normal([3, 128], mean=6, stddev=0.1, seed = 1),\n",
    "              tf.random_normal([3, 128], mean=1, stddev=1, seed = 1),\n",
    "              tf.random_normal([3, 128], mean=3, stddev=4, seed = 1))\n",
    "    loss = triplet_loss(y_true, y_pred)\n",
    "    \n",
    "    print(\"loss = \" + str(loss.eval()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step3: Compiling Model & Loading Weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_weights_from_FaceNet(FRmodel):\n",
    "    ## WEIGHTS - list of all the weights names\n",
    "    weights = WEIGHTS\n",
    "    weights_dict = load_weights()\n",
    "\n",
    "    # Set layer weights of the model\n",
    "    for name in weights:\n",
    "        if FRmodel.get_layer(name) != None:\n",
    "            FRmodel.get_layer(name).set_weights(weights_dict[name])\n",
    "        elif model.get_layer(name) != None:\n",
    "            model.get_layer(name).set_weights(weights_dict[name])\n",
    "\n",
    "def load_weights():\n",
    "    # Set weights path\n",
    "    dirPath = './weights'\n",
    "    fileNames = filter(lambda f: not f.startswith('.'), os.listdir(dirPath))\n",
    "    paths = {}\n",
    "    weights_dict = {}\n",
    "\n",
    "    for n in fileNames:\n",
    "        paths[n.replace('.csv', '')] = dirPath + '/' + n\n",
    "\n",
    "    for name in WEIGHTS:\n",
    "        if 'conv' in name:\n",
    "            conv_w = genfromtxt(paths[name + '_w'], delimiter=',', dtype=None)\n",
    "            conv_w = np.reshape(conv_w, conv_shape[name])\n",
    "            conv_w = np.transpose(conv_w, (2, 3, 1, 0))\n",
    "            conv_b = genfromtxt(paths[name + '_b'], delimiter=',', dtype=None)\n",
    "            weights_dict[name] = [conv_w, conv_b]     \n",
    "        elif 'bn' in name:\n",
    "            bn_w = genfromtxt(paths[name + '_w'], delimiter=',', dtype=None)\n",
    "            bn_b = genfromtxt(paths[name + '_b'], delimiter=',', dtype=None)\n",
    "            bn_m = genfromtxt(paths[name + '_m'], delimiter=',', dtype=None)\n",
    "            bn_v = genfromtxt(paths[name + '_v'], delimiter=',', dtype=None)\n",
    "            weights_dict[name] = [bn_w, bn_b, bn_m, bn_v]\n",
    "        elif 'dense' in name:\n",
    "            dense_w = genfromtxt(dirPath+'/dense_w.csv', delimiter=',', dtype=None)\n",
    "            dense_w = np.reshape(dense_w, (128, 736))\n",
    "            dense_w = np.transpose(dense_w, (1, 0))\n",
    "            dense_b = genfromtxt(dirPath+'/dense_b.csv', delimiter=',', dtype=None)\n",
    "            weights_dict[name] = [dense_w, dense_b]\n",
    "\n",
    "    return weights_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "FRmodel.compile(optimizer = 'adam', loss = triplet_loss, metrics = ['accuracy'])\n",
    "load_weights_from_FaceNet(FRmodel)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step4: Encoding Images "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def img_to_encoding1(image_path, model):\n",
    "    img1 = cv2.imread(image_path, 1)\n",
    "    dim = (96,96)\n",
    "    img1 = cv2.resize(img1,dim, interpolation = cv2.INTER_AREA)\n",
    "    img = img1[...,::-1]\n",
    "    img = np.around(np.transpose(img, (2,0,1))/255.0, decimals=12)\n",
    "    x_train = np.array([img])\n",
    "    embedding = model.predict_on_batch(x_train)\n",
    "    return embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step5: Verifying model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def verify(image_path, identity, database, model):\n",
    "    \n",
    "    #encoding for the image.\n",
    "    encoding = img_to_encoding1(image_path,model)\n",
    "    # distance with identity'\n",
    "    dist = np.linalg.norm(encoding - database[identity])\n",
    "\n",
    "    if dist < 0.7:\n",
    "        print(\"It's \" + str(identity) + \", Welcome to Bank!!!\")\n",
    "        door_open = True\n",
    "    else:\n",
    "        print(\"It's not \" + str(identity) + \", Call the Police!!!\")\n",
    "        door_open = False\n",
    "        \n",
    "    return dist, door_open"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using One-Shot Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "database = {}\n",
    "database[\"HJC_Peshchan\"] = img_to_encoding1(\"genuine/NFI-00101001.png\", FRmodel)\n",
    "database[\"Vermututr\"] = img_to_encoding1(\"genuine/NFI-00201002.png\", FRmodel)\n",
    "database[\"Yuan\"] = img_to_encoding1(\"genuine/NFI-00301003.png\", FRmodel)\n",
    "database[\"Cbugn\"] = img_to_encoding1(\"genuine/NFI-00401004.png\", FRmodel)\n",
    "database[\"GJ\"] = img_to_encoding1(\"genuine/NFI-00501005.png\", FRmodel)\n",
    "database[\"Mary_Van_Camp\"] = img_to_encoding1(\"genuine/NFI-00601006.png\", FRmodel)\n",
    "database[\"SUruin\"] = img_to_encoding1(\"genuine/NFI-00701007.png\", FRmodel)\n",
    "database[\"Hmceycey\"] = img_to_encoding1(\"genuine/NFI-00801008.png\", FRmodel)\n",
    "database[\"Pramod\"] = img_to_encoding1(\"genuine/NFI-00901009.png\", FRmodel)\n",
    "\n",
    "database[\"Paul_J\"] = img_to_encoding1(\"genuine/NFI-02303023.png\", FRmodel)\n",
    "database[\"Myuan_Priyush\"] = img_to_encoding1(\"genuine/NFI-02404024.png\", FRmodel)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Verify** for one shot learning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"Genuine.png\"> Original Image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"Forged.png\"> Forged Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "It's not HJC_Peshchan, Call the Police!!!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.7450517, False)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "verify(\"forged/NFI-00301001.png\", \"HJC_Peshchan\", database, FRmodel)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"Genuine1.png\"> Genuine Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "It's HJC_Peshchan, Welcome to Bank!!!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.27836695, True)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "verify(\"genuine/NFI-00104001.png\", \"HJC_Peshchan\", database, FRmodel)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"YuanO.png\"> Original Image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"YuanF.png\"> Forged Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "It's Yuan, Welcome to Bank!!!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.47795907, True)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "verify(\"forged/NFI-00405003.png\", \"Yuan\", database, FRmodel)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"YuanG.png\"> Genuine Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "It's Yuan, Welcome to Bank!!!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.46050698, True)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "verify(\"genuine/NFI-00302003.png\", \"Yuan\", database, FRmodel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Able to detect subtle differences!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.1040557 ,  0.14639431, -0.07380492, -0.09834596,  0.04153858,\n",
       "         0.11747582, -0.03230847,  0.04555031, -0.0156483 , -0.06735273,\n",
       "        -0.01550347, -0.03578096,  0.02269405, -0.00298959, -0.03529763,\n",
       "         0.01043521,  0.02132434, -0.13309786, -0.14070717,  0.15531158,\n",
       "         0.04126226,  0.07089272,  0.03167374,  0.01084924, -0.10053892,\n",
       "        -0.01308829, -0.02469324, -0.03853963,  0.01978707, -0.00459529,\n",
       "        -0.08815479, -0.0243421 , -0.05690145,  0.07646212, -0.07781163,\n",
       "         0.09322509,  0.03752657,  0.18610103, -0.11345245,  0.07004084,\n",
       "        -0.06997257,  0.04245119, -0.09933359, -0.04784032,  0.01983559,\n",
       "         0.1984514 ,  0.09372989,  0.01937401, -0.12868862,  0.16689777,\n",
       "        -0.00308439,  0.14334612, -0.11759578,  0.08946506,  0.12416398,\n",
       "        -0.08096603,  0.02606221,  0.01811345,  0.01859047, -0.13931578,\n",
       "        -0.0040506 ,  0.05017831,  0.10614674, -0.09526715, -0.03956392,\n",
       "        -0.15489021,  0.07712438,  0.19285089,  0.05657451,  0.0994212 ,\n",
       "         0.10514449,  0.038941  ,  0.02034371,  0.02292089,  0.06925685,\n",
       "         0.06306645, -0.0484712 ,  0.12332118,  0.0129399 , -0.01124636,\n",
       "         0.03679262,  0.1435186 , -0.03943245, -0.0217624 , -0.00441468,\n",
       "         0.03866751, -0.00460896,  0.06141011,  0.07197866,  0.09793225,\n",
       "         0.03229719, -0.17305376, -0.0536084 ,  0.03892547, -0.01877438,\n",
       "        -0.05135889,  0.10943783, -0.06057118,  0.02052311, -0.05011811,\n",
       "         0.01795658,  0.06282662, -0.09052713,  0.09058681, -0.24101794,\n",
       "         0.04388534, -0.1841917 ,  0.0108988 , -0.04738968, -0.03157669,\n",
       "         0.14069068,  0.1647519 , -0.07462292,  0.07456081,  0.00545527,\n",
       "         0.05350582, -0.16955441, -0.15209883, -0.07511719,  0.00260327,\n",
       "        -0.05307909,  0.13288222,  0.16504435,  0.10877549,  0.05948703,\n",
       "        -0.00472249,  0.0320701 , -0.1154303 ]], dtype=float32)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img_to_encoding1(\"forged/NFI-00401003.png\", FRmodel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.11717176,  0.04423836, -0.11427275, -0.03848497,  0.14286608,\n",
       "         0.05382209,  0.06395762,  0.00444713,  0.00554071, -0.0871586 ,\n",
       "         0.00803049,  0.05278345,  0.0861866 ,  0.09689719, -0.00292422,\n",
       "        -0.08748735,  0.10283022, -0.10693506, -0.11507187,  0.20999338,\n",
       "         0.02240222, -0.00395892,  0.07314583,  0.0421036 , -0.02757388,\n",
       "        -0.07125108, -0.05095497, -0.01434501,  0.0224327 ,  0.03930177,\n",
       "        -0.00660794, -0.01289282, -0.01572733,  0.04408295, -0.07684384,\n",
       "         0.07413968,  0.00299342,  0.24544401, -0.14409228, -0.01436194,\n",
       "         0.00410425,  0.13664557, -0.11182234, -0.00612561, -0.15001889,\n",
       "         0.11832305,  0.01601231,  0.0328695 ,  0.01195212,  0.09618542,\n",
       "        -0.06390809,  0.10124411, -0.07943651,  0.01180394,  0.127187  ,\n",
       "        -0.07303889, -0.0930092 ,  0.13251705, -0.01785541, -0.09231742,\n",
       "        -0.09570184, -0.0072075 ,  0.12259635, -0.13481224,  0.00625706,\n",
       "        -0.0890718 ,  0.08139711,  0.19156003, -0.02500153,  0.03426458,\n",
       "         0.17513584,  0.09745094,  0.03699801,  0.01960605,  0.02553211,\n",
       "         0.09840952, -0.02260128,  0.03252774,  0.03611822,  0.03345126,\n",
       "        -0.0579004 ,  0.06482745, -0.01032139, -0.03943697,  0.04801007,\n",
       "        -0.07365248,  0.04689277,  0.09054097,  0.06931379, -0.00046961,\n",
       "         0.03912631, -0.15074833,  0.02565292, -0.01109741,  0.0109597 ,\n",
       "        -0.18949284,  0.08348695, -0.05516652,  0.00797959, -0.08766153,\n",
       "        -0.03232819,  0.06883081,  0.00820597,  0.08805735, -0.29697374,\n",
       "         0.07323442, -0.1347382 ,  0.01572794,  0.08692807, -0.02513942,\n",
       "         0.1311897 ,  0.13207717, -0.11410511,  0.14226656, -0.00491234,\n",
       "         0.04010302, -0.1402364 , -0.11986528, -0.05887217,  0.09993411,\n",
       "        -0.09665087,  0.11610002, -0.00577802,  0.12838195,  0.0101555 ,\n",
       "         0.00617585, -0.01085773, -0.07596861]], dtype=float32)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img_to_encoding1(\"genuine/NFI-00103001.png\", FRmodel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = img_to_encoding1(\"genuine/NFI-00102001.png\", FRmodel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "b = img_to_encoding1(\"genuine/NFI-00103001.png\", FRmodel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "a1 = np.array([[1,2]])\n",
    "b1 = np.array([[1,2]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1.15688615e-01,  3.20313871e-02, -9.27501768e-02,\n",
       "        -5.37407696e-02,  1.32184938e-01,  8.10448602e-02,\n",
       "         6.96275905e-02,  3.45241986e-02, -3.45069170e-02,\n",
       "        -8.24541599e-02, -1.30007956e-02,  5.49336709e-02,\n",
       "         8.09830427e-02,  9.75692272e-02,  1.33590894e-02,\n",
       "        -1.02557443e-01,  1.09500021e-01, -1.02464393e-01,\n",
       "        -8.63782167e-02,  1.96165651e-01,  9.97928437e-03,\n",
       "         1.34713855e-02,  7.86842629e-02,  5.81782982e-02,\n",
       "        -4.74950634e-02, -7.36115873e-02, -2.86515281e-02,\n",
       "        -3.21474671e-03,  1.35241505e-02,  2.07400415e-02,\n",
       "        -2.81391088e-02, -5.59066832e-02, -1.63930133e-02,\n",
       "         5.76668084e-02, -7.34250247e-02,  6.56741410e-02,\n",
       "         6.78066211e-03,  2.17143476e-01, -1.77916348e-01,\n",
       "        -1.66024528e-02,  7.67319556e-03,  1.33220896e-01,\n",
       "        -9.18806568e-02,  2.86759040e-03, -1.66994482e-01,\n",
       "         1.15551919e-01,  2.51331553e-02,  3.63404155e-02,\n",
       "        -1.94187313e-02,  1.22499630e-01, -4.45030928e-02,\n",
       "         6.26097172e-02, -7.94366300e-02,  1.92958862e-02,\n",
       "         1.46072656e-01, -5.26440628e-02, -9.64876562e-02,\n",
       "         1.51571602e-01, -4.13064845e-04, -9.45976526e-02,\n",
       "        -9.32598710e-02,  2.04712078e-02,  1.23248786e-01,\n",
       "        -1.49889857e-01,  3.42887864e-02, -8.03603455e-02,\n",
       "         6.51374608e-02,  1.60995737e-01, -5.24248406e-02,\n",
       "         6.24314472e-02,  1.55207232e-01,  8.60425532e-02,\n",
       "         3.87124345e-02,  2.78658997e-02,  2.00031772e-02,\n",
       "         1.03173241e-01, -4.65067662e-03,  3.94623652e-02,\n",
       "         6.48163557e-02, -7.14138150e-05, -6.17340803e-02,\n",
       "         3.97303179e-02, -6.18875399e-03, -5.03766090e-02,\n",
       "         4.02174592e-02, -9.46014374e-02,  4.68172804e-02,\n",
       "         9.04800147e-02,  7.24997222e-02,  4.13103476e-02,\n",
       "         5.27480915e-02, -1.49791211e-01,  2.43982784e-02,\n",
       "        -1.24571156e-02,  2.27007382e-02, -1.61849290e-01,\n",
       "         1.00585185e-01, -4.13586795e-02,  2.10056594e-03,\n",
       "        -7.66546056e-02, -2.42489949e-02,  1.01196781e-01,\n",
       "         6.30214904e-03,  5.96778616e-02, -3.01337481e-01,\n",
       "         7.14595914e-02, -1.36459261e-01, -1.41675863e-02,\n",
       "         5.47268838e-02, -3.60922553e-02,  1.49468362e-01,\n",
       "         1.33528262e-01, -7.08838180e-02,  1.27126023e-01,\n",
       "        -7.67649617e-04,  3.31249461e-02, -1.07586741e-01,\n",
       "        -1.14616126e-01, -6.06069118e-02,  1.04422688e-01,\n",
       "        -8.95630717e-02,  8.24121088e-02,  3.75110889e-04,\n",
       "         1.08339973e-01,  1.09071676e-02,  1.87006649e-02,\n",
       "         4.91038617e-03, -8.97799432e-02]], dtype=float32)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(a+b)/2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:tensorflow]",
   "language": "python",
   "name": "conda-env-tensorflow-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
